# llamaindex+Internlm2 RAGå®è·µ

## ç¬”è®°

æ­¤æ–‡æ¡£å¯èƒ½æ­¥éª¤è¾ƒå¿«ï¼Œæœ‰ä»»ä½•ä¸æ˜ç™½çš„å¯ç›´æ¥ç§èŠæˆ‘æˆ–è€…ç»™æˆ‘ç•™è¨€ï¼Œè°¢è°¢~

### å…ˆç®€å•äº†è§£ä¸‹ä»€ä¹ˆæ˜¯RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼‰æŠ€æœ¯æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”ŸæˆåŠŸèƒ½çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ã€‚å®ƒé€šè¿‡ä»å¤§å‹å¤–éƒ¨æ•°æ®åº“ä¸­æ£€ç´¢ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œæ¥è¾…åŠ©ç”Ÿæˆæ¨¡å‹å›ç­”é—®é¢˜ï¼Œè¿™ç§æŠ€æœ¯ä¸»è¦ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹ç”Ÿæˆç­‰åº”ç”¨ä¸­éå¸¸æœ‰æ•ˆã€‚RAGæŠ€æœ¯é€šè¿‡æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å¾—ç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ ä¸°å¯Œã€å‡†ç¡®å’Œç›¸å…³ã€‚RAGæŠ€æœ¯çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¼ ç»Ÿçš„æ£€ç´¢æŠ€æœ¯ä¸ç°ä»£çš„è‡ªç„¶è¯­è¨€ç”ŸæˆæŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥æé«˜æ–‡æœ¬ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚

![](.\image-lx\1.png)

RAGæ¨¡å‹çš„åŸºæœ¬åŸç†æ˜¯å°†æ£€ç´¢å’Œç”Ÿæˆä¸¤ç§æŠ€æœ¯ç»“åˆèµ·æ¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆæ–‡æœ¬ä¹‹å‰è®¿é—®å¹¶åˆ©ç”¨å¤§é‡å¤–éƒ¨ä¿¡æ¯ã€‚æ£€ç´¢ç»„ä»¶è´Ÿè´£ä»ä¸€ä¸ªå¤§å‹çš„çŸ¥è¯†åº“ä¸­æ£€ç´¢å‡ºä¸ç»™å®šè¾“å…¥ç›¸å…³çš„ä¿¡æ¯ï¼Œè¿™ä¸ªçŸ¥è¯†åº“å¯ä»¥æ˜¯ç»´åŸºç™¾ç§‘ã€ä¸“ä¸šæœŸåˆŠã€ä¹¦ç±ç­‰ä»»ä½•å½¢å¼çš„æ–‡æ¡£é›†åˆã€‚ç”Ÿæˆç»„ä»¶åˆ™æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„Transformeræ¨¡å‹ï¼ˆå¦‚GPTæˆ–BERTï¼‰ï¼Œå®ƒç»“åˆäº†åŸå§‹è¾“å…¥å’Œæ£€ç´¢ç»„ä»¶æä¾›çš„å¤–éƒ¨ä¿¡æ¯æ¥ç”Ÿæˆæ–‡æœ¬ã€‚

ç»™æ¨¡å‹æ³¨å…¥æ–°çŸ¥è¯†çš„æ–¹å¼ï¼Œå¯ä»¥ç®€å•åˆ†ä¸ºä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯å†…éƒ¨çš„ï¼Œå³æ›´æ–°æ¨¡å‹çš„æƒé‡ï¼Œå¦ä¸€ä¸ªå°±æ˜¯å¤–éƒ¨çš„æ–¹å¼ï¼Œç»™æ¨¡å‹æ³¨å…¥æ ¼å¤–çš„ä¸Šä¸‹æ–‡æˆ–è€…è¯´å¤–éƒ¨ä¿¡æ¯ï¼Œä¸æ”¹å˜å®ƒçš„çš„æƒé‡ã€‚

æ€»ç»“ä¸‹æ¥å°±æ˜¯ä¸€ç§éœ€è¦é‡æ–°è®­ç»ƒæ›´æ–°æ¨¡å‹çš„æƒé‡ï¼Œå¦ä¸€ç§å°±æ˜¯é€šè¿‡å¤–éƒ¨æ³¨å…¥ç„¶åæ£€ç´¢çš„å½¢å¼ä¸éœ€è¦æ›´æ–°æ¨¡å‹æƒé‡å°±èƒ½æŒæ¡æ–°é¢†åŸŸçš„çŸ¥è¯†ã€‚

**ç‰¹ç‚¹ï¼š**

ä¸éœ€è¦é‡æ–°è®­ç»ƒå°±å¯ä»¥è·å¾—æ–°çš„ä¿¡æ¯æ„ŸçŸ¥ï¼Œæé«˜äº†æ¨¡å‹å†…å®¹å›å¤çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†å†…å®¹å®æ—¶æ›´æ–°æ€§é—®é¢˜å¹¶ä¸”æœ‰æ•ˆçš„ç¼“è§£äº†å¹»è§‰é—®é¢˜ã€‚

## LlamaIndex HuggingFaceLLM

è¿™é‡Œæ–°å»ºä¸€ä¸ªå¼€å‘æœºé€‰æ‹©30%A100*1çš„èµ„æºï¼Œé•œåƒä½¿ç”¨cuda11.7-condaæ“ä½œï¼Œåˆ›å»ºåæˆ‘ä»¬è¿›å…¥å¼€å‘æœºæ–°å»ºä¸€ä¸ªcondaè™šæ‹Ÿç¯å¢ƒï¼Œåå­—éšä¾¿å–ï¼Œæˆ‘è¿™é‡Œä¹Ÿæ˜¯è·Ÿç€æ•™ç¨‹æ¥çš„ã€‚

```python
conda create -n llamaindex python=3.10
conda activate llamaindex
conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia
pip install einops
pip install  protobuf
#  å®‰è£… Llamaindex
conda activate llamaindex
pip install llama-index==0.10.38 llama-index-llms-huggingface==0.2.0 "transformers[torch]==4.41.1" "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1 sentence-transformers==2.7.0 sentencepiece==0.2.0
```

æ¥ä¸‹æ¥æˆ‘å°±ç›´æ¥å‘½ä»¤ä¸€æ­¥ä¸€æ­¥æ‰§è¡Œå•¦

```python
cd ~
mkdir llamaindex_demo model
cd ~/llamaindex_demo
touch download_hf.py
```



download_hf.pyå†…å®¹ï¼š

```python
import os

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ä¸‹è½½æ¨¡å‹
os.system('huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/model/sentence-transformer')
```



ç»§ç»­æ‰§è¡Œåé¢çš„å‘½ä»¤ï¼ˆæˆ‘å°±ä¸ä¸€ä¸€è§£é‡Šäº†ï¼Œæœ‰ä»»ä½•ä¸æ‡‚çš„å¯ä»¥ç™¾åº¦ï¼Œæ˜¯å®åœ¨ä¸è¡Œç›´æ¥ç§æˆ‘ï¼‰

```python
cd /root/llamaindex_demo
python download_hf.py
# ä¸‹è½½NTKèµ„æº
cd /root
git clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
cd nltk_data
mv packages/*  ./
cd tokenizers
unzip punkt.zip
cd ../taggers
unzip averaged_perceptron_tagger.zip
# åˆ›å»ºè½¯é“¾æ¥ï¼ˆèŠ‚çœèµ„æºï¼‰
cd ~/llamaindex_demo
touch llamaindex_internlm.py
cd ~/llamaindex_demo
touch llamaindex_internlm.py
```

llamaindex_internlm.pyä»£ç ï¼š

```python
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.llms import ChatMessage
llm = HuggingFaceLLM(
    model_name="/root/model/internlm2-chat-1_8b",
    tokenizer_name="/root/model/internlm2-chat-1_8b",
    model_kwargs={"trust_remote_code":True},
    tokenizer_kwargs={"trust_remote_code":True}
)

rsp = llm.chat(messages=[ChatMessage(content="xtuneræ˜¯ä»€ä¹ˆï¼Ÿ")])
print(rsp)
```

ç»§ç»­æ‰§è¡Œï¼š

```python
conda activate llamaindex
cd ~/llamaindex_demo/
python llamaindex_internlm.py
```

![](.\image-lx\4.png)

æˆ‘ä»¬å‘ç°internlm2-chat-1_8bæ¨¡å‹å¹¶ä¸çŸ¥é“xtunerå…·ä½“æ˜¯ä»€ä¹ˆï¼Œå›ç­”ä¹Ÿæ˜¯åŸºäºå·²ç»è®­ç»ƒçš„çŸ¥è¯†åº“çš„å†…å®¹å»æè¿°ï¼Œå¹¶æ²¡æœ‰è¾¾åˆ°æˆ‘ä»¬æƒ³è¦çš„æ•ˆæœï¼Œæ¥ä¸‹æ¥å°±ç»“åˆRAGçš„æ–¹å¼æ¥æŸ¥çœ‹æ•ˆæœã€‚

## LlamaIndex RAG

å®‰è£…LlamaIndexè¯å‘é‡çš„ä¾èµ–ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
conda activate llamaindex
pip install llama-index-embeddings-huggingface llama-index-embeddings-instructor
cd ~/llamaindex_demo
# è·å–çŸ¥è¯†åº“
mkdir data
cd data
git clone https://github.com/InternLM/xtuner.git
mv xtuner/README_zh-CN.md ./
cd ~/llamaindex_demo
touch llamaindex_RAG.py
```



llamaindex_RAG.pyå†…å®¹å¦‚ä¸‹ï¼š


```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM

#åˆå§‹åŒ–ä¸€ä¸ªHuggingFaceEmbeddingå¯¹è±¡ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
embed_model = HuggingFaceEmbedding(
#æŒ‡å®šäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„sentence-transformeræ¨¡å‹çš„è·¯å¾„
    model_name="/root/model/sentence-transformer"
)
#å°†åˆ›å»ºçš„åµŒå…¥æ¨¡å‹èµ‹å€¼ç»™å…¨å±€è®¾ç½®çš„embed_modelå±æ€§ï¼Œ
#è¿™æ ·åœ¨åç»­çš„ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­å°±ä¼šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚
Settings.embed_model = embed_model

llm = HuggingFaceLLM(
    model_name="/root/model/internlm2-chat-1_8b",
    tokenizer_name="/root/model/internlm2-chat-1_8b",
    model_kwargs={"trust_remote_code":True},
    tokenizer_kwargs={"trust_remote_code":True}
)
#è®¾ç½®å…¨å±€çš„llmå±æ€§ï¼Œè¿™æ ·åœ¨ç´¢å¼•æŸ¥è¯¢æ—¶ä¼šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚
Settings.llm = llm

#ä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜ä¸­
documents = SimpleDirectoryReader("/root/llamaindex_demo/data").load_data()
#åˆ›å»ºä¸€ä¸ªVectorStoreIndexï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„æ–‡æ¡£æ¥æ„å»ºç´¢å¼•ã€‚
# æ­¤ç´¢å¼•å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å‚¨è¿™äº›å‘é‡ä»¥ä¾¿äºå¿«é€Ÿæ£€ç´¢ã€‚
index = VectorStoreIndex.from_documents(documents)
# åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ï¼Œè¿™ä¸ªå¼•æ“å¯ä»¥æ¥æ”¶æŸ¥è¯¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£çš„å“åº”ã€‚
query_engine = index.as_query_engine()
response = query_engine.query("xtuneræ˜¯ä»€ä¹ˆ?")

print(response)
```



æ¥ä¸‹æ¥æ‰§è¡Œè¿è¡Œè¿™ä¸ªè„šæœ¬å³å¯ï¼š

```python
conda activate llamaindex
cd ~/llamaindex_demo/
python llamaindex_RAG.py
```



è¿è¡Œç»“æœå¦‚ä¸‹ï¼š

![](.\image-lx\5.png)

##  LlamaIndex web

è¿è¡Œä¸€ä¸‹å‘½ä»¤ï¼š

```python
pip install streamlit==1.36.0
cd ~/llamaindex_demo
touch app.py
```



åœ¨`app.py`æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š

```python
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM

st.set_page_config(page_title="llama_index_demo", page_icon="ğŸ¦œğŸ”—")
st.title("llama_index_demo")

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def init_models():
    embed_model = HuggingFaceEmbedding(
        model_name="/root/model/sentence-transformer"
    )
    Settings.embed_model = embed_model

    llm = HuggingFaceLLM(
        model_name="/root/model/internlm2-chat-1_8b",
        tokenizer_name="/root/model/internlm2-chat-1_8b",
        model_kwargs={"trust_remote_code": True},
        tokenizer_kwargs={"trust_remote_code": True}
    )
    Settings.llm = llm

    documents = SimpleDirectoryReader("/root/llamaindex_demo/data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    return query_engine

# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if 'query_engine' not in st.session_state:
    st.session_state['query_engine'] = init_models()

def greet2(question):
    response = st.session_state['query_engine'].query(question)
    return response

      
# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]    

    # Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]

st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Function for generating LLaMA2 response
def generate_llama_index_response(prompt_input):
    return greet2(prompt_input)

# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Gegenerate_llama_index_response last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama_index_response(prompt)
            placeholder = st.empty()
            placeholder.markdown(response)
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
```



æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨streamlitæ¡†æ¶å¿«é€Ÿæ„å»ºä¸€ä¸ªwebåº”ç”¨ï¼Œç„¶åæœ¬åœ°win+ræ‰§è¡Œ`ssh -CNg -L 8501:127.0.0.1:8501 root@ssh.intern-ai.org.cn -p 44415`ï¼Œè¿™é‡Œ44415æ˜¯æˆ‘çš„å¼€å‘æœºsshç«¯å£å·ï¼Œè¯·ä¿®æ”¹ä¸ºä½ è‡ªå·±InternStudioä¸Šé¢SSHè¿æ¥çš„ç«¯å£å·å³å¯ã€‚å¦‚ä¸‹å›¾æ˜¾ç¤º

![](.\image-lx\6.png)

## æ€»ç»“

è¿™èŠ‚å°å†…å®¹ä¸»è¦è¿˜æ˜¯å®ç°RAGæŠ€æœ¯å¸®åŠ©æ¨¡å‹æœ‰æ•ˆçš„ç¼“è§£å¹»è§‰ï¼Œé‡‡ç”¨çš„ä¾æ—§æ˜¯`internlm2-chat-1_8b`è¿™ä¸ªbaseæ¨¡å‹ï¼Œæ¯”è¾ƒé€‚åˆåˆšå¼€å§‹çš„ç†Ÿæ‚‰RAGï¼Œä½†æ˜¯è¿™ä¸ªbase modelè¿˜æ˜¯æœ‰ç‚¹åŸºç¡€ï¼Œé™¤äº†æˆ‘ä»¬ç†Ÿæ‚‰çš„ä»£ç ç†è§£å¤–ï¼Œè¿˜æ˜¯ä½¿ç”¨äº†LLama_indexåº“ä¸­çš„ `HuggingFaceLLM` ç±»å’Œbaseæ¨¡å‹äº¤äº’ï¼Œå¹¶ä¸”å…è®¸æ¨¡å‹åŠ è½½å’Œæ‰§è¡Œè¿œç¨‹ä»£ç ï¼Œåœ¨RAGä»£ç ä¸­ï¼Œè¿˜ä½¿ç”¨äº†llama_indexæ„å»ºä¸€ä¸ªæ–‡æœ¬æœç´¢å¼•æ“ï¼Œå°†æˆ‘ä»¬æŸ¥è¯¢çš„æ–‡æœ¬ï¼ˆâ€Xtuneræ˜¯ä»€ä¹ˆâ€œï¼‰è½¬æ¢æˆå‘é‡å¹¶ä¸”æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œé‡‡ç”¨çš„æ˜¯sentence-transformerè½¬æ¢æˆçš„å‘é‡è¡¨ç¤ºï¼ŒåŸºäºå¥å­æ¥åµŒå…¥çš„ï¼Œä½¿ç”¨ `SimpleDirectoryReader` ä»æŒ‡å®šç›®å½•ï¼ˆ`/root/llamaindex_demo/data`ï¼‰è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜ä¸­ï¼Œæœ€ååˆ›å»ºç´¢å¼•ã€åˆ›å»ºæŸ¥è¯¢å¼•æ“å®ŒæˆRAGçš„å®ç°ï¼Œæˆ‘è§‰å¾—è¾“å‡ºæœ‰ç‚¹æ…¢ï¼Œå¯èƒ½æ˜¯gpuå—é™å§ã€‚
